{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9a6b3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\3 Stars\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "16df7558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "11686c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "11e21352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一',\n",
       " '一下',\n",
       " '一些',\n",
       " '一切',\n",
       " '一则',\n",
       " '一天',\n",
       " '一定',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一时',\n",
       " '一来',\n",
       " '一样',\n",
       " '一次',\n",
       " '一片',\n",
       " '一直',\n",
       " '一致',\n",
       " '一般',\n",
       " '一起',\n",
       " '一边',\n",
       " '一面',\n",
       " '万一',\n",
       " '上下',\n",
       " '上升',\n",
       " '上去',\n",
       " '上来',\n",
       " '上述',\n",
       " '上面',\n",
       " '下列',\n",
       " '下去',\n",
       " '下来',\n",
       " '下面',\n",
       " '不一',\n",
       " '不久',\n",
       " '不仅',\n",
       " '不会',\n",
       " '不但',\n",
       " '不光',\n",
       " '不单',\n",
       " '不变',\n",
       " '不只',\n",
       " '不可',\n",
       " '不同',\n",
       " '不够',\n",
       " '不如',\n",
       " '不得',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不敢',\n",
       " '不断',\n",
       " '不是',\n",
       " '不比',\n",
       " '不然',\n",
       " '不特',\n",
       " '不独',\n",
       " '不管',\n",
       " '不能',\n",
       " '不要',\n",
       " '不论',\n",
       " '不足',\n",
       " '不过',\n",
       " '不问',\n",
       " '与',\n",
       " '与其',\n",
       " '与否',\n",
       " '与此同时',\n",
       " '专门',\n",
       " '且',\n",
       " '两者',\n",
       " '严格',\n",
       " '严重',\n",
       " '个',\n",
       " '个人',\n",
       " '个别',\n",
       " '中小',\n",
       " '中间',\n",
       " '丰富',\n",
       " '临',\n",
       " '为',\n",
       " '为主',\n",
       " '为了',\n",
       " '为什么',\n",
       " '为什麽',\n",
       " '为何',\n",
       " '为着',\n",
       " '主张',\n",
       " '主要',\n",
       " '举行',\n",
       " '乃',\n",
       " '乃至',\n",
       " '么',\n",
       " '之',\n",
       " '之一',\n",
       " '之前',\n",
       " '之后',\n",
       " '之後',\n",
       " '之所以',\n",
       " '之类',\n",
       " '乌乎',\n",
       " '乎',\n",
       " '乘',\n",
       " '也',\n",
       " '也好',\n",
       " '也是',\n",
       " '也罢',\n",
       " '了',\n",
       " '了解',\n",
       " '争取',\n",
       " '于',\n",
       " '于是',\n",
       " '于是乎',\n",
       " '云云',\n",
       " '互相',\n",
       " '产生',\n",
       " '人们',\n",
       " '人家',\n",
       " '什么',\n",
       " '什么样',\n",
       " '什麽',\n",
       " '今后',\n",
       " '今天',\n",
       " '今年',\n",
       " '今後',\n",
       " '仍然',\n",
       " '从',\n",
       " '从事',\n",
       " '从而',\n",
       " '他',\n",
       " '他人',\n",
       " '他们',\n",
       " '他的',\n",
       " '代替',\n",
       " '以',\n",
       " '以上',\n",
       " '以下',\n",
       " '以为',\n",
       " '以便',\n",
       " '以免',\n",
       " '以前',\n",
       " '以及',\n",
       " '以后',\n",
       " '以外',\n",
       " '以後',\n",
       " '以来',\n",
       " '以至',\n",
       " '以至于',\n",
       " '以致',\n",
       " '们',\n",
       " '任',\n",
       " '任何',\n",
       " '任凭',\n",
       " '任务',\n",
       " '企图',\n",
       " '伟大',\n",
       " '似乎',\n",
       " '似的',\n",
       " '但',\n",
       " '但是',\n",
       " '何',\n",
       " '何况',\n",
       " '何处',\n",
       " '何时',\n",
       " '作为',\n",
       " '你',\n",
       " '你们',\n",
       " '你的',\n",
       " '使得',\n",
       " '使用',\n",
       " '例如',\n",
       " '依',\n",
       " '依照',\n",
       " '依靠',\n",
       " '促进',\n",
       " '保持',\n",
       " '俺',\n",
       " '俺们',\n",
       " '倘',\n",
       " '倘使',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '假使',\n",
       " '假如',\n",
       " '假若',\n",
       " '做到',\n",
       " '像',\n",
       " '允许',\n",
       " '充分',\n",
       " '先后',\n",
       " '先後',\n",
       " '先生',\n",
       " '全部',\n",
       " '全面',\n",
       " '兮',\n",
       " '共同',\n",
       " '关于',\n",
       " '其',\n",
       " '其一',\n",
       " '其中',\n",
       " '其二',\n",
       " '其他',\n",
       " '其余',\n",
       " '其它',\n",
       " '其实',\n",
       " '其次',\n",
       " '具体',\n",
       " '具体地说',\n",
       " '具体说来',\n",
       " '具有',\n",
       " '再者',\n",
       " '再说',\n",
       " '冒',\n",
       " '冲',\n",
       " '决定',\n",
       " '况且',\n",
       " '准备',\n",
       " '几',\n",
       " '几乎',\n",
       " '几时',\n",
       " '凭',\n",
       " '凭借',\n",
       " '出去',\n",
       " '出来',\n",
       " '出现',\n",
       " '分别',\n",
       " '则',\n",
       " '别',\n",
       " '别的',\n",
       " '别说',\n",
       " '到',\n",
       " '前后',\n",
       " '前者',\n",
       " '前进',\n",
       " '前面',\n",
       " '加之',\n",
       " '加以',\n",
       " '加入',\n",
       " '加强',\n",
       " '十分',\n",
       " '即',\n",
       " '即令',\n",
       " '即使',\n",
       " '即便',\n",
       " '即或',\n",
       " '即若',\n",
       " '却不',\n",
       " '原来',\n",
       " '又',\n",
       " '及',\n",
       " '及其',\n",
       " '及时',\n",
       " '及至',\n",
       " '双方',\n",
       " '反之',\n",
       " '反应',\n",
       " '反映',\n",
       " '反过来',\n",
       " '反过来说',\n",
       " '取得',\n",
       " '受到',\n",
       " '变成',\n",
       " '另',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '只是',\n",
       " '只有',\n",
       " '只要',\n",
       " '只限',\n",
       " '叫',\n",
       " '叫做',\n",
       " '召开',\n",
       " '叮咚',\n",
       " '可',\n",
       " '可以',\n",
       " '可是',\n",
       " '可能',\n",
       " '可见',\n",
       " '各',\n",
       " '各个',\n",
       " '各人',\n",
       " '各位',\n",
       " '各地',\n",
       " '各种',\n",
       " '各级',\n",
       " '各自',\n",
       " '合理',\n",
       " '同',\n",
       " '同一',\n",
       " '同时',\n",
       " '同样',\n",
       " '后来',\n",
       " '后面',\n",
       " '向',\n",
       " '向着',\n",
       " '吓',\n",
       " '吗',\n",
       " '否则',\n",
       " '吧',\n",
       " '吧哒',\n",
       " '吱',\n",
       " '呀',\n",
       " '呃',\n",
       " '呕',\n",
       " '呗',\n",
       " '呜',\n",
       " '呜呼',\n",
       " '呢',\n",
       " '周围',\n",
       " '呵',\n",
       " '呸',\n",
       " '呼哧',\n",
       " '咋',\n",
       " '和',\n",
       " '咚',\n",
       " '咦',\n",
       " '咱',\n",
       " '咱们',\n",
       " '咳',\n",
       " '哇',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '哉',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎哟',\n",
       " '哗',\n",
       " '哟',\n",
       " '哦',\n",
       " '哩',\n",
       " '哪',\n",
       " '哪个',\n",
       " '哪些',\n",
       " '哪儿',\n",
       " '哪天',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪样',\n",
       " '哪边',\n",
       " '哪里',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '唉',\n",
       " '啊',\n",
       " '啐',\n",
       " '啥',\n",
       " '啦',\n",
       " '啪达',\n",
       " '喂',\n",
       " '喏',\n",
       " '喔唷',\n",
       " '嗡嗡',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '嗳',\n",
       " '嘎',\n",
       " '嘎登',\n",
       " '嘘',\n",
       " '嘛',\n",
       " '嘻',\n",
       " '嘿',\n",
       " '因',\n",
       " '因为',\n",
       " '因此',\n",
       " '因而',\n",
       " '固然',\n",
       " '在',\n",
       " '在下',\n",
       " '地',\n",
       " '坚决',\n",
       " '坚持',\n",
       " '基本',\n",
       " '处理',\n",
       " '复杂',\n",
       " '多',\n",
       " '多少',\n",
       " '多数',\n",
       " '多次',\n",
       " '大力',\n",
       " '大多数',\n",
       " '大大',\n",
       " '大家',\n",
       " '大批',\n",
       " '大约',\n",
       " '大量',\n",
       " '失去',\n",
       " '她',\n",
       " '她们',\n",
       " '她的',\n",
       " '好的',\n",
       " '好象',\n",
       " '如',\n",
       " '如上所述',\n",
       " '如下',\n",
       " '如何',\n",
       " '如其',\n",
       " '如果',\n",
       " '如此',\n",
       " '如若',\n",
       " '存在',\n",
       " '宁',\n",
       " '宁可',\n",
       " '宁愿',\n",
       " '宁肯',\n",
       " '它',\n",
       " '它们',\n",
       " '它们的',\n",
       " '它的',\n",
       " '安全',\n",
       " '完全',\n",
       " '完成',\n",
       " '实现',\n",
       " '实际',\n",
       " '宣布',\n",
       " '容易',\n",
       " '密切',\n",
       " '对',\n",
       " '对于',\n",
       " '对应',\n",
       " '将',\n",
       " '少数',\n",
       " '尔后',\n",
       " '尚且',\n",
       " '尤其',\n",
       " '就',\n",
       " '就是',\n",
       " '就是说',\n",
       " '尽',\n",
       " '尽管',\n",
       " '属于',\n",
       " '岂但',\n",
       " '左右',\n",
       " '巨大',\n",
       " '巩固',\n",
       " '己',\n",
       " '已经',\n",
       " '帮助',\n",
       " '常常',\n",
       " '并',\n",
       " '并不',\n",
       " '并不是',\n",
       " '并且',\n",
       " '并没有',\n",
       " '广大',\n",
       " '广泛',\n",
       " '应当',\n",
       " '应用',\n",
       " '应该',\n",
       " '开外',\n",
       " '开始',\n",
       " '开展',\n",
       " '引起',\n",
       " '强烈',\n",
       " '强调',\n",
       " '归',\n",
       " '当',\n",
       " '当前',\n",
       " '当时',\n",
       " '当然',\n",
       " '当着',\n",
       " '形成',\n",
       " '彻底',\n",
       " '彼',\n",
       " '彼此',\n",
       " '往',\n",
       " '往往',\n",
       " '待',\n",
       " '後来',\n",
       " '後面',\n",
       " '得',\n",
       " '得出',\n",
       " '得到',\n",
       " '心里',\n",
       " '必然',\n",
       " '必要',\n",
       " '必须',\n",
       " '怎',\n",
       " '怎么',\n",
       " '怎么办',\n",
       " '怎么样',\n",
       " '怎样',\n",
       " '怎麽',\n",
       " '总之',\n",
       " '总是',\n",
       " '总的来看',\n",
       " '总的来说',\n",
       " '总的说来',\n",
       " '总结',\n",
       " '总而言之',\n",
       " '恰恰相反',\n",
       " '您',\n",
       " '意思',\n",
       " '愿意',\n",
       " '慢说',\n",
       " '成为',\n",
       " '我',\n",
       " '我们',\n",
       " '我的',\n",
       " '或',\n",
       " '或是',\n",
       " '或者',\n",
       " '战斗',\n",
       " '所',\n",
       " '所以',\n",
       " '所有',\n",
       " '所谓',\n",
       " '打',\n",
       " '扩大',\n",
       " '把',\n",
       " '抑或',\n",
       " '拿',\n",
       " '按',\n",
       " '按照',\n",
       " '换句话说',\n",
       " '换言之',\n",
       " '据',\n",
       " '掌握',\n",
       " '接着',\n",
       " '接著',\n",
       " '故',\n",
       " '故此',\n",
       " '整个',\n",
       " '方便',\n",
       " '方面',\n",
       " '旁人',\n",
       " '无宁',\n",
       " '无法',\n",
       " '无论',\n",
       " '既',\n",
       " '既是',\n",
       " '既然',\n",
       " '时候',\n",
       " '明显',\n",
       " '明确',\n",
       " '是',\n",
       " '是否',\n",
       " '是的',\n",
       " '显然',\n",
       " '显著',\n",
       " '普通',\n",
       " '普遍',\n",
       " '更加',\n",
       " '曾经',\n",
       " '替',\n",
       " '最后',\n",
       " '最大',\n",
       " '最好',\n",
       " '最後',\n",
       " '最近',\n",
       " '最高',\n",
       " '有',\n",
       " '有些',\n",
       " '有关',\n",
       " '有利',\n",
       " '有力',\n",
       " '有所',\n",
       " '有效',\n",
       " '有时',\n",
       " '有点',\n",
       " '有的',\n",
       " '有着',\n",
       " '有著',\n",
       " '望',\n",
       " '朝',\n",
       " '朝着',\n",
       " '本',\n",
       " '本着',\n",
       " '来',\n",
       " '来着',\n",
       " '极了',\n",
       " '构成',\n",
       " '果然',\n",
       " '果真',\n",
       " '某',\n",
       " '某个',\n",
       " '某些',\n",
       " '根据',\n",
       " '根本',\n",
       " '欢迎',\n",
       " '正在',\n",
       " '正如',\n",
       " '正常',\n",
       " '此',\n",
       " '此外',\n",
       " '此时',\n",
       " '此间',\n",
       " '毋宁',\n",
       " '每',\n",
       " '每个',\n",
       " '每天',\n",
       " '每年',\n",
       " '每当',\n",
       " '比',\n",
       " '比如',\n",
       " '比方',\n",
       " '比较',\n",
       " '毫不',\n",
       " '没有',\n",
       " '沿',\n",
       " '沿着',\n",
       " '注意',\n",
       " '深入',\n",
       " '清楚',\n",
       " '满足',\n",
       " '漫说',\n",
       " '焉',\n",
       " '然则',\n",
       " '然后',\n",
       " '然後',\n",
       " '然而',\n",
       " '照',\n",
       " '照着',\n",
       " '特别是',\n",
       " '特殊',\n",
       " '特点',\n",
       " '现代',\n",
       " '现在',\n",
       " '甚么',\n",
       " '甚而',\n",
       " '甚至',\n",
       " '用',\n",
       " '由',\n",
       " '由于',\n",
       " '由此可见',\n",
       " '的',\n",
       " '的话',\n",
       " '目前',\n",
       " '直到',\n",
       " '直接',\n",
       " '相似',\n",
       " '相信',\n",
       " '相反',\n",
       " '相同',\n",
       " '相对',\n",
       " '相对而言',\n",
       " '相应',\n",
       " '相当',\n",
       " '相等',\n",
       " '省得',\n",
       " '看出',\n",
       " '看到',\n",
       " '看来',\n",
       " '看看',\n",
       " '看见',\n",
       " '真是',\n",
       " '真正',\n",
       " '着',\n",
       " '着呢',\n",
       " '矣',\n",
       " '知道',\n",
       " '确定',\n",
       " '离',\n",
       " '积极',\n",
       " '移动',\n",
       " '突出',\n",
       " '突然',\n",
       " '立即',\n",
       " '第',\n",
       " '等',\n",
       " '等等',\n",
       " '管',\n",
       " '紧接着',\n",
       " '纵',\n",
       " '纵令',\n",
       " '纵使',\n",
       " '纵然',\n",
       " '练习',\n",
       " '组成',\n",
       " '经',\n",
       " '经常',\n",
       " '经过',\n",
       " '结合',\n",
       " '结果',\n",
       " '给',\n",
       " '绝对',\n",
       " '继续',\n",
       " '继而',\n",
       " '维持',\n",
       " '综上所述',\n",
       " '罢了',\n",
       " '考虑',\n",
       " '者',\n",
       " '而',\n",
       " '而且',\n",
       " '而况',\n",
       " '而外',\n",
       " '而已',\n",
       " '而是',\n",
       " '而言',\n",
       " '联系',\n",
       " '能',\n",
       " '能否',\n",
       " '能够',\n",
       " '腾',\n",
       " '自',\n",
       " '自个儿',\n",
       " '自从',\n",
       " '自各儿',\n",
       " '自家',\n",
       " '自己',\n",
       " '自身',\n",
       " '至',\n",
       " '至于',\n",
       " '良好',\n",
       " '若',\n",
       " '若是',\n",
       " '若非',\n",
       " '范围',\n",
       " '莫若',\n",
       " '获得',\n",
       " '虽',\n",
       " '虽则',\n",
       " '虽然',\n",
       " '虽说',\n",
       " '行为',\n",
       " '行动',\n",
       " '表明',\n",
       " '表示',\n",
       " '被',\n",
       " '要',\n",
       " '要不',\n",
       " '要不是',\n",
       " '要不然',\n",
       " '要么',\n",
       " '要是',\n",
       " '要求',\n",
       " '规定',\n",
       " '觉得',\n",
       " '认为',\n",
       " '认真',\n",
       " '认识',\n",
       " '让',\n",
       " '许多',\n",
       " '论',\n",
       " '设使',\n",
       " '设若',\n",
       " '该',\n",
       " '说明',\n",
       " '诸位',\n",
       " '谁',\n",
       " '谁知',\n",
       " '赶',\n",
       " '起',\n",
       " '起来',\n",
       " '起见',\n",
       " '趁',\n",
       " '趁着',\n",
       " '越是',\n",
       " '跟',\n",
       " '转动',\n",
       " '转变',\n",
       " '转贴',\n",
       " '较',\n",
       " '较之',\n",
       " '边',\n",
       " '达到',\n",
       " '迅速',\n",
       " '过',\n",
       " '过去',\n",
       " '过来',\n",
       " '运用',\n",
       " '还是',\n",
       " '还有',\n",
       " '这',\n",
       " '这个',\n",
       " '这么',\n",
       " '这么些',\n",
       " '这么样',\n",
       " '这么点儿',\n",
       " '这些',\n",
       " '这会儿',\n",
       " '这儿',\n",
       " '这就是说',\n",
       " '这时',\n",
       " '这样',\n",
       " '这点',\n",
       " '这种',\n",
       " '这边',\n",
       " '这里',\n",
       " '这麽',\n",
       " '进入',\n",
       " '进步',\n",
       " '进而',\n",
       " '进行',\n",
       " '连',\n",
       " '连同',\n",
       " '适应',\n",
       " '适当',\n",
       " '适用',\n",
       " '逐步',\n",
       " '逐渐',\n",
       " '通常',\n",
       " '通过',\n",
       " '造成',\n",
       " '遇到',\n",
       " '遭到',\n",
       " '避免',\n",
       " '那',\n",
       " '那个',\n",
       " '那么',\n",
       " '那么些',\n",
       " '那么样',\n",
       " '那些',\n",
       " '那会儿',\n",
       " '那儿',\n",
       " '那时',\n",
       " '那样',\n",
       " '那边',\n",
       " '那里',\n",
       " '那麽',\n",
       " '部分',\n",
       " '鄙人',\n",
       " '采取',\n",
       " '里面',\n",
       " '重大',\n",
       " '重新',\n",
       " '重要',\n",
       " '鉴于',\n",
       " '问题',\n",
       " '防止',\n",
       " '阿',\n",
       " '附近',\n",
       " '限制',\n",
       " '除',\n",
       " '除了',\n",
       " '除此之外',\n",
       " '除非',\n",
       " '随',\n",
       " '随着',\n",
       " '随著',\n",
       " '集中',\n",
       " '需要',\n",
       " '非但',\n",
       " '非常',\n",
       " '非徒',\n",
       " '靠',\n",
       " '顺',\n",
       " '顺着',\n",
       " '首先',\n",
       " '高兴',\n",
       " '是不是']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"chinese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc230a5",
   "metadata": {},
   "source": [
    "Apply Tokenization, Porter Stemming, Stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3fd1368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'The goal of NLP is to enable machines to read, understand, and derive meaning from human language in a valuable way.', 'It involves several key tasks such as text classification, sentiment analysis, machine translation, and question answering.', 'Before applying these techniques, however, it is important to clean and preprocess the text.', 'This includes steps like converting to lowercase, removing punctuation, eliminating stop words, and stemming or lemmatization.', 'By removing common stop words such as \"the\", \"is\", \"and\", and \"to\", we can reduce noise and focus on the more meaningful parts of the text for analysis.']\n",
      "['natur languag process ( nlp ) branch artifici intellig focus interact comput human natur languag .', 'the goal nlp enabl machin read , understand , deriv mean human languag valuabl way .', 'it involv sever key task text classif , sentiment analysi , machin translat , question answer .', 'befor appli techniqu , howev , import clean preprocess text .', 'thi includ step like convert lowercas , remov punctuat , elimin stop word , stem lemmat .', \"by remov common stop word `` '' , `` '' , `` '' , `` '' , reduc nois focu meaning part text analysi .\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\3 Stars\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import pickle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "corpus=\"\"\" Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. The goal of NLP is to enable machines to read, understand, and derive meaning from human language in a valuable way. It involves several key tasks such as text classification, sentiment analysis, machine translation, and question answering. Before applying these techniques, however, it is important to clean and preprocess the text. This includes steps like converting to lowercase, removing punctuation, eliminating stop words, and stemming or lemmatization. By removing common stop words such as \"the\", \"is\", \"and\", and \"to\", we can reduce noise and focus on the more meaningful parts of the text for analysis.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "stemming=PorterStemmer()\n",
    "\n",
    "\n",
    "# Use Treebank tokenizer directly (used under the hood by word_tokenize)\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Load tokenizer directly from the file\n",
    "with open(r\"C:\\Users\\3 Stars Laptop\\Desktop\\Ghufran\\NLP\\tokenizers\\punkt\\english.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Use it\n",
    "\n",
    "sentences = tokenizer.tokenize(corpus)\n",
    "print(sentences)\n",
    "\n",
    "## Apply Stopwords Filter and then Apply PorterStemming.\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenizer.tokenize(sentences[i])  # Tokenize the sentence\n",
    "    words = [stemming.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]  # Apply stemming and remove stopwords\n",
    "    sentences[i] = \" \".join(words)  # Reconstruct the sentence\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04283eb0",
   "metadata": {},
   "source": [
    "Apply Tokenization, SnowBallStemming, Stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "46a8e180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\3 Stars\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'The goal of NLP is to enable machines to read, understand, and derive meaning from human language in a valuable way.', 'It involves several key tasks such as text classification, sentiment analysis, machine translation, and question answering.', 'Before applying these techniques, however, it is important to clean and preprocess the text.', 'This includes steps like converting to lowercase, removing punctuation, eliminating stop words, and stemming or lemmatization.', 'By removing common stop words such as \"the\", \"is\", \"and\", and \"to\", we can reduce noise and focus on the more meaningful parts of the text for analysis.']\n",
      "['natur languag process ( nlp ) branch artifici intellig focus interact comput human natur languag .', 'the goal nlp enabl machin read , understand , deriv mean human languag valuabl way .', 'it involv sever key task text classif , sentiment analysi , machin translat , question answer .', 'befor appli techniqu , howev , import clean preprocess text .', 'this includ step like convert lowercas , remov punctuat , elimin stop word , stem lemmat .', \"by remov common stop word `` '' , `` '' , `` '' , `` '' , reduc nois focus meaning part text analysi .\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import pickle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download(\"stopwords\")\n",
    "corpus=\"\"\" Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. The goal of NLP is to enable machines to read, understand, and derive meaning from human language in a valuable way. It involves several key tasks such as text classification, sentiment analysis, machine translation, and question answering. Before applying these techniques, however, it is important to clean and preprocess the text. This includes steps like converting to lowercase, removing punctuation, eliminating stop words, and stemming or lemmatization. By removing common stop words such as \"the\", \"is\", \"and\", and \"to\", we can reduce noise and focus on the more meaningful parts of the text for analysis.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "snow_stemming=SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# Use Treebank tokenizer directly (used under the hood by word_tokenize)\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Load tokenizer directly from the file\n",
    "with open(r\"C:\\Users\\3 Stars Laptop\\Desktop\\Ghufran\\NLP\\tokenizers\\punkt\\english.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Use it\n",
    "\n",
    "sentences = tokenizer.tokenize(corpus)\n",
    "print(sentences)\n",
    "## Apply Stopwords Filter and then Apply SnowBallStemmingStemming.\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenizer.tokenize(sentences[i])  # Tokenize the sentence\n",
    "    words = [snow_stemming.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]  # Apply stemming and remove stopwords\n",
    "    sentences[i] = \" \".join(words)  # Reconstruct the sentence\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3535b",
   "metadata": {},
   "source": [
    "Apply Tokenization, Lemmatization, Stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0e712423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\3 Stars\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\3 Stars\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'The goal of NLP is to enable machines to read, understand, and derive meaning from human language in a valuable way.', 'It involves several key tasks such as text classification, sentiment analysis, machine translation, and question answering.', 'Before applying these techniques, however, it is important to clean and preprocess the text.', 'This includes steps like converting to lowercase, removing punctuation, eliminating stop words, and stemming or lemmatization.', 'By removing common stop words such as \"the\", \"is\", \"and\", and \"to\", we can reduce noise and focus on the more meaningful parts of the text for analysis.']\n",
      "['Natural Language Processing ( NLP ) branch artificial intelligence focus interaction computer human natural language .', 'The goal NLP enable machine read , understand , derive meaning human language valuable way .', 'It involves several key task text classification , sentiment analysis , machine translation , question answering .', 'Before applying technique , however , important clean preprocess text .', 'This includes step like converting lowercase , removing punctuation , eliminating stop word , stemming lemmatization .', \"By removing common stop word `` '' , `` '' , `` '' , `` '' , reduce noise focus meaningful part text analysis .\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\3 Stars\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # For multilingual WordNet\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import pickle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download(\"stopwords\")\n",
    "corpus=\"\"\" Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. The goal of NLP is to enable machines to read, understand, and derive meaning from human language in a valuable way. It involves several key tasks such as text classification, sentiment analysis, machine translation, and question answering. Before applying these techniques, however, it is important to clean and preprocess the text. This includes steps like converting to lowercase, removing punctuation, eliminating stop words, and stemming or lemmatization. By removing common stop words such as \"the\", \"is\", \"and\", and \"to\", we can reduce noise and focus on the more meaningful parts of the text for analysis.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Use Treebank tokenizer directly (used under the hood by word_tokenize)\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Load tokenizer directly from the file\n",
    "with open(r\"C:\\Users\\3 Stars Laptop\\Desktop\\Ghufran\\NLP\\tokenizers\\punkt\\english.pickle\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Use it\n",
    "\n",
    "sentences = tokenizer.tokenize(corpus)\n",
    "print(sentences)\n",
    "## Apply Stopwords Filter and then Apply Lemmatization.\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenizer.tokenize(sentences[i])  # Tokenize the sentence\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]  # Apply stemming and remove stopwords\n",
    "    sentences[i] = \" \".join(words)  # Reconstruct the sentence\n",
    "\n",
    "print(sentences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
